{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extract_features.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4xxkis7d12b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import VGG16\n",
        "from keras.applications import imagenet_utils\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import load_img\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import progressbar\n",
        "import random\n",
        "import os\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOsdthrTeb6-",
        "colab_type": "code",
        "outputId": "eba041e6-fb4a-4423-97f9-31adca122909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# mount Google Drive\n",
        "# note that authorization code might be required\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q16HRnoHidoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add local libraries to system path\n",
        "# hdf5_dataset_writer.py should be visible to the interpreter\n",
        "# eg. placed in My Drive/Colab Notebooks/utilities/io\n",
        "sys.path.append('/content/gdrive/My Drive/Colab Notebooks/utilities/io')\n",
        "from hdf5_dataset_writer import HDF5DatasetWriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL5p_s9vee26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set paths\n",
        "datasetPath = r'/content/gdrive/My Drive/KrakN/database'\n",
        "outputPath = r\"/content/gdrive/My Drive/KrakN/database/features\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7uLI-TbegBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set/check dataset & output path, delete previous output if exists\n",
        "if not os.path.exists(datasetPath):\n",
        "    print(\"Dataset at {}\\nDoes not exist!\\nQuitting now\".format(datasetPath))\n",
        "    quit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOvgjhRQehhD",
        "colab_type": "code",
        "outputId": "3f2b26f3-80d5-4fea-eff5-baf60d1f38d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "batchSize = 256\n",
        "bufferSize = 1000\n",
        "\n",
        "# load images and shuffle them\n",
        "print(\"Loading images...\")\n",
        "imagePaths = list(paths.list_images(datasetPath))\n",
        "random.shuffle(imagePaths)\n",
        "print(\"{} images loaded\".format(len(imagePaths)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading images...\n",
            "22 images loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KmXtnmyel3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get scale factor\n",
        "splitted = imagePaths[0].split('_')\n",
        "scale = splitted[-1]\n",
        "scale = scale[:-4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhIQxj7ylJsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract class labels and encode them\n",
        "labels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aii5qdfolK_9",
        "colab_type": "code",
        "outputId": "dcae5ce5-103e-49a5-cd5c-1dac4df6cb3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load VGG16 network excluding final FC layers\n",
        "print(\"loading network...\")\n",
        "model = VGG16(weights=\"imagenet\", include_top=False)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading network...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W5nddz-lL4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize dataset writer\n",
        "dataset = HDF5DatasetWriter((len(imagePaths), 512 * 7 * 7), outputPath + \"_s_{}\".format(scale) + '.hdf5', \"features\", bufferSize)\n",
        "dataset.storeClassLabels(le.classes_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdvwErtHlM1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize progress bar\n",
        "widgets = [\"Extracting Features: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
        "pbar = progressbar.ProgressBar(maxval=len(imagePaths), widgets=widgets).start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3onTlDxepF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loop over images\n",
        "for i in np.arange(0, len(imagePaths), batchSize):\n",
        "    batchPaths = imagePaths[i:i + batchSize]\n",
        "    batchLabels = labels[i:i + batchSize]\n",
        "    batchImages = []\n",
        "\n",
        "    for (j, imagePath) in enumerate(batchPaths):\n",
        "        # load and resize image\n",
        "        image = load_img(imagePath, target_size=(224, 224))\n",
        "        image = img_to_array(image)\n",
        "\n",
        "        # preprocess image by expanding and subtracting mean RGB value\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        image = imagenet_utils.preprocess_input(image)\n",
        "\n",
        "        # add image to batch\n",
        "        batchImages.append(image)\n",
        "\n",
        "    # pass images thr network\n",
        "    batchImages = np.vstack(batchImages)\n",
        "    features = model.predict(batchImages, batch_size=batchSize)\n",
        "\n",
        "    # reshape features\n",
        "    features = features.reshape((features.shape[0], 512 * 7 * 7))\n",
        "\n",
        "    # add features and labels to dataset\n",
        "    dataset.add(features, batchLabels)\n",
        "    pbar.update(i)\n",
        "\n",
        "dataset.close()\n",
        "pbar.finish()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}